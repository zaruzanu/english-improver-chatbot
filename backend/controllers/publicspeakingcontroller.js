const fs = require('fs');
const path = require('path');
const { transcribeAudio } = require('../utils/whisperutils');
const { convertWebmToMp3 } = require('../utils/ffmpegutils');
const { textToSpeech } = require('../utils/ttsutils');
const PublicSpeaking = require('../models/publicspeaking');
const OpenAI = require('openai');

// âœ… OpenAI Setup
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

exports.publicSpeaking = async (req, res) => {
  try {
    console.log('\nğŸ¤ [ROUTE HIT] POST /api/publicspeaking/public-speaking');
    console.log('ğŸ“ Uploaded File:', req.file?.originalname || 'None');
    console.log('ğŸ“§ Email in Request:', req.body.email || 'None');

    // âœ… Clean email and tone
    const rawEmail = req.body.email || 'guest';
    const userEmail = rawEmail.replace(/[^a-zA-Z0-9]/g, '_') || 'guest';
    const selectedTone = req.body.tone || 'formal';
    console.log(`ğŸ‘¤ Email: ${userEmail}`);
    // âœ… Uploaded webm file path
    const webmPath = req.file?.path;
    if (!webmPath) {
      console.error('âŒ No audio file provided');
      return res.status(400).json({ error: 'No audio file provided' });
    }

    console.log(`ğŸ“¥ Uploaded file path: ${webmPath}`);

    // âœ… Convert to mp3
    const mp3Path = `${webmPath}.mp3`;
    await convertWebmToMp3(webmPath, mp3Path);
    console.log(`ğŸ¼ Converted to mp3: ${mp3Path}`);

    // âœ… Transcription using Whisper
    console.log(`ğŸ“¥ Transcribing: ${mp3Path}`);
    const transcript = await transcribeAudio(mp3Path);
    console.log(`ğŸ“ Transcription result: ${transcript}`);

    // âœ… Professional tone rephrasing
    const professionalVersion = await generateSpeakingFeedback(transcript, selectedTone);
    console.log(`ğŸ§  Rephrased (Tone: ${selectedTone}): ${professionalVersion}`);

    // âœ… Ensure audio folder exists
    const audioDir = path.join(__dirname, '../public/audio');
    if (!fs.existsSync(audioDir)) {
      fs.mkdirSync(audioDir, { recursive: true });
      console.log('ğŸ“‚ Created audio folder');
    }

    // âœ… Generate TTS
    const ttsFileName = `${userEmail}-public-speaking-response.mp3`;
    const ttsFilePath = path.join(audioDir, ttsFileName);
    console.log(`ğŸ—£ï¸ Starting TTS for: ${professionalVersion}`);
    await textToSpeech(professionalVersion, ttsFilePath);
    console.log(`ğŸ”Š TTS saved at: ${ttsFilePath}`);

    // âœ… URLs
    const userAudioUrl = `http://localhost:5000/${webmPath.replace('uploads/', '')}`;
    const aiAudioUrl = `http://localhost:5000/audio/${ttsFileName}`;

    // âœ… Save to DB
    await PublicSpeaking.create({
  email: userEmail,
  userTranscription: transcript,     // âœ… match your schema
  aiFeedback: professionalVersion,   // âœ… match your schema
  audioPath: webmPath,
  userAudioUrl,
  aiAudioUrl
});
   console.log('ğŸ’¾ Session saved to MongoDB');

    // âœ… Send frontend JSON
    res.json({
      transcription: transcript,
      professionalVersion,
      userAudioUrl,
      aiAudioUrl
    });

    console.log('âœ… Response sent successfully.');

  } catch (err) {
    console.error('âŒ Error in publicSpeaking controller:', err);
    res.status(500).json({ error: 'Internal server error' });
  }
};

// âœ… Helper: Generate professional rephrasing with selected tone
async function generateSpeakingFeedback(transcript, tone = 'formal') {
  const prompt = `Convert the following speech into a ${tone} tone. Make it sound confident, clear, and professional:\n\n"${transcript}"`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.7,
    });

    return response.choices[0].message.content.trim();
  } catch (err) {
    console.error('âŒ GPT Rephrase Error:', err.message);
    return transcript;
  }
}
